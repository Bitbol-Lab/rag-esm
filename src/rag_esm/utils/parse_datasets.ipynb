{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import os\n",
    "import Bio.SeqIO\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from rag_esm.utils.hamming import *\n",
    "import torch\n",
    "\n",
    "path = input(\"Path to the 'OpenProteinSet_uniclust30-filtered' dataset (downloaded from aws):\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.multiprocessing as mp\n",
    "    \n",
    "def gen(lst):\n",
    "    for el in lst:\n",
    "        try:\n",
    "            # read all sequences from a3m file\n",
    "            with open(path + f\"/{el}/a3m/uniclust30.a3m\", \"r\") as f:    \n",
    "                data = [(record.description, str(record.seq)) for record in Bio.SeqIO.parse(f, \"fasta\")]\n",
    "            sequences = [x[1] for x in data]\n",
    "            seq_ids = [x[0] for x in data]\n",
    "            # from each seq id get the PE score (protein existence)\n",
    "            pes = [x.split(\"PE=\")[1].split(\" \")[0] if \"PE=\" in x else None for x in seq_ids]\n",
    "            pes = [int(x) if (x !=\"\") and x is not None else None for x in pes]\n",
    "            # val, cnt = np.unique(pes, return_counts=True)\n",
    "            # add msa to list of clusters\n",
    "            assert len(sequences) > 0\n",
    "            assert len(sequences) == len(pes)\n",
    "        except:\n",
    "            # print(f\"Failed to parse {el}\")\n",
    "            # go to next cluster\n",
    "            continue\n",
    "        # compute hamming distance between sequences\n",
    "        # check how much memory is available on gpu\n",
    "        device=\"cuda:1\"\n",
    "        free_memory, total_memory = torch.cuda.mem_get_info(device)\n",
    "        if free_memory < 10*1024**3:\n",
    "            device = \"cpu\"\n",
    "        \n",
    "        hamming_distances = hamming_gpu(sequences, return_matrix=True, normalize=False, batch=100, device=device)\n",
    "        # get closest sequences\n",
    "        num = len(sequences)//10\n",
    "        val, idx = torch.topk(hamming_distances, num, dim=1, largest=False, sorted=True)\n",
    "        # yield sample\n",
    "        yield {\"cluster_id\": el,\n",
    "                \"msa\": np.array(sequences),\n",
    "                \"PE_scores\": np.array(pes),\n",
    "                \"closest_sequences\": idx[:,1:].numpy()}\n",
    "\n",
    "def make_dataset_and_save_it_to_disk(path, debug=True, save_path=f\"../../../data/example_dataset\", list_test_ids=None):\n",
    "    \"\"\"\n",
    "    Create a dataset from the given path and save it to disk.\n",
    "    \"\"\"\n",
    "    def get_folders(path):\n",
    "        # get all folders in given path\n",
    "        p = pathlib.Path(path)\n",
    "        return [os.path.basename(x) for x in p.iterdir() if x.is_dir()]\n",
    "    \n",
    "    # get all clusters folders\n",
    "    lst = get_folders(path)\n",
    "    if list_test_ids is not None:\n",
    "        # get the test clusters as the overlap between the given list and the clusters\n",
    "        test_clusters = set(x for x in list_test_ids if x in lst)\n",
    "        train_clusters = set(x for x in lst if x not in test_clusters)\n",
    "        assert len(test_clusters.intersection(train_clusters)) == 0, \"Test clusters are in train clusters!\"\n",
    "        # split train_clusters in train and validation clusters\n",
    "        test_size = len(test_clusters)\n",
    "        train_clusters = list(train_clusters)\n",
    "        np.random.shuffle(train_clusters)\n",
    "        train_clusters, val_clusters = set(train_clusters[:-test_size]), set(train_clusters[-test_size:])\n",
    "        assert len(train_clusters.intersection(val_clusters)) == 0, \"Train and validation clusters overlap!\"\n",
    "        assert set(lst) == test_clusters.union(train_clusters.union(val_clusters)), \"Some clusters are missing!\"\n",
    "        print(\"All tests passed!\")\n",
    "        print(f\"Train clusters: {len(train_clusters)}, Validation clusters: {len(val_clusters)}, Test clusters: {len(test_clusters)}\")\n",
    "    else:\n",
    "        test_size = int(input(\"Enter the test size as the number of clusters to use as test set: \"))\n",
    "        train_clusters = set(lst[:32]) if debug else set(lst[:-2*test_size])\n",
    "        val_clusters = set(lst[32:32+test_size]) if debug else set(lst[-2*test_size:-test_size])\n",
    "        test_clusters = set(lst[32+test_size:32+2*test_size]) if debug else set(lst[-test_size:])\n",
    "        lst = list(train_clusters) + list(test_clusters) + list(val_clusters)\n",
    "    train_clusters, val_clusters, test_clusters = list(train_clusters), list(val_clusters), list(test_clusters)\n",
    "        \n",
    "    signature = datasets.Features({\"cluster_id\": datasets.Value(\"string\"),\n",
    "                                   \"msa\": datasets.Sequence(datasets.Value(\"string\")),\n",
    "                                   \"PE_scores\": datasets.Sequence(datasets.Value(\"int64\")),\n",
    "                                   \"closest_sequences\": datasets.Sequence(datasets.Sequence(datasets.Value(\"int64\")))})\n",
    "    counter = []\n",
    "    print(\"Creating dataset from generator...\")\n",
    "    ds_train = datasets.Dataset.from_generator(gen,\n",
    "                                               features=signature,\n",
    "                                               gen_kwargs={\"lst\": train_clusters},\n",
    "                                               num_proc=80\n",
    "                                               )\n",
    "    ds_val = datasets.Dataset.from_generator(gen,\n",
    "                                               features=signature,\n",
    "                                               gen_kwargs={\"lst\": val_clusters},\n",
    "                                               num_proc=min(80, len(val_clusters))\n",
    "                                               )\n",
    "    ds_test = datasets.Dataset.from_generator(gen,\n",
    "                                               features=signature,\n",
    "                                               gen_kwargs={\"lst\": test_clusters},\n",
    "                                               num_proc=min(80, len(test_clusters))\n",
    "                                               )\n",
    "    print(\"Length of train dataset: \", len(ds_train), \"Length of validation dataset: \", len(ds_val), \"Length of test dataset: \", len(ds_test))\n",
    "    print(\"Done!\")\n",
    "    \n",
    "    # create a dataset from the list of clusters\n",
    "    # ds_train = datasets.Dataset.from_dict(all_clusters[\"train\"])\n",
    "    # test_set = datasets.Dataset.from_dict(all_clusters[\"test\"])\n",
    "    # Split the dataset into training, test, and validation sets\n",
    "    # train_val_split = ds_train.train_test_split(test_size=test_size)\n",
    "    # train_set = train_val_split['train']\n",
    "    # val_set = train_val_split['test']\n",
    "    \n",
    "    # make a DatasetDict\n",
    "    dataset_dict = datasets.DatasetDict({\"train\": ds_train, \"val\": ds_val, \"test\": ds_test})\n",
    "    # Save it to disk\n",
    "    dataset_dict.save_to_disk(save_path, max_shard_size=\"1GB\")\n",
    "    del ds_train, ds_val, ds_test, dataset_dict\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    mp.set_start_method('spawn', force=True)  \n",
    "    # make_dataset_and_save_it_to_disk(path, debug=True, save_path=f\"../../../data/example_dataset\")\n",
    "    list_test_ids = None\n",
    "    make_dataset_and_save_it_to_disk(path,\n",
    "                                    debug=True,\n",
    "                                    save_path=f\"../../../data/OpenProteinSet_uniclust30-filtered_rag-esm\",\n",
    "                                    list_test_ids=list_test_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "import datasets\n",
    "\n",
    "ds = datasets.load_from_disk(\"../../../data/OpenProteinSet_uniclust30-filtered_rag-esm\")[\"train\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-esm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
